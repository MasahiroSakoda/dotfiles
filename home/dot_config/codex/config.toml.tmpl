# -*-mode:toml-*- vim:ft=toml.gotexttmpl

# Model Selection: https://github.com/openai/codex/blob/main/docs/config.md#model-selection
model = "gpt5-codex"
model_reasoning_effort  = "medium"
model_reasoning_summary = "auto"

[profiles.default]
model = "gpt5"
model_provider = "openai"
approval_policy = "never"
model_reasoning_effort = "high"
model_reasoning_summary = "detailed"

# Execution environment: https://github.com/openai/codex/blob/main/docs/config.md#execution-environment
approval_policy = "on-failure" # options: `untrusted`, `on-failure`, `on-request`, `never`
sandbox_mode    = "read-only"  # options: `read-only`, `workspace-write`, `danger-full-access`

# MCP server config
{{- range $key, $value := includeTemplate "common/ai/mcp-servers.json.tmpl" . | fromJson }}
[mcp_servers.{{ $key }}]
{{- if $value | hasKey "type" }}
{{-   $_ := $value | deleteValueAtPath "type" }}
{{- end }}
{{- if and ($value | hasKey "env") (eq (len $value.env) 0) }}
{{-   $_ := $value | deleteValueAtPath "env" }}
{{- end }}
{{ $value | toToml | trim }}
{{ end }}

[history]
persistence = "save-all" # options: `none`, `save-all`
